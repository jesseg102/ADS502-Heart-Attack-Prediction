---
title: "ADS 502 - Heart Attack Classification"
author: "Halee Staggs, Vicky van der Wagt, Jesse Gutierrez"
date: "2023-03-16"
output: pdf_document
---



```{r}
# Install required packages
#install.packages('tidyverse')
#install.packages('dplyr')
#install.packages('randomForest')
#install.packages('ggeffects')
#install.packages(pROC)

#load package libraries
library(pROC)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(tidyverse)
library(caret)
library(C50)
library(rpart)
library(e1071)
library(nnet)
library(NeuralNetTools)
library(data.table)
library(dplyr)
library(randomForest)
library(ggeffects)
library(sjPlot)
library(sjlabelled)
library(sjmisc)
```
# The Data does not have any missing values. 

```{r}
#read in data file
data <- read_csv("heart.csv")

#examine data types and descriptives
summary(data)
describe(data)

#view dataset 
#View(data)

```






# Some variables need to be updated to factors because they are categorical/ordinal
```{r}
#update discrete/binary variables to factors
data$sex <- factor(data$sex)
data$cp <- factor(data$cp)
data$fbs <- factor(data$fbs)
data$restecg <- factor(data$restecg)
data$exng <- factor(data$exng)
data$slp <- factor(data$slp)
data$caa <- factor(data$caa)
data$output <- factor(data$output)
data$thall <- factor(data$thall)

#reexamine data summary for data types
#summary(data)
#describe(data)

head(data)
```


# Replace numerical categories with actual cateogires.
## Even though they were already converted to factors, can look back on categories for reference

```{r}
#chest pain type
data$cp_cat<-c("0"="typical_angina","1"="atypical angina","3"="non-anginal pain","4"="asymptomatic")[data$cp]

#false if fasting blood sugar <=120mg/dl, otherwise true
data$fbs_cat<-c("0"="false","1"="true")[data$fbs]

#resting electrocardiographic results
data$restecg_cat<-c("0"="normal","1"="ST-T wave abnormality", "2" = "ventricular hypertrophy")[data$restecg]

#exercise induced angina
data$exng_cat<-c("0"="no","1"="yes")[data$exng]

#slope of peak exercise ST segment
data$slp_cat<-c("0"="unsloping","1"="flat", "2"="downsloaing")[data$slp]

#thalassemia
data$thall_cat<-c("0"="null","1"="fixed defect", "2"="normal", "3"="reversable defect")[data$thall]

#thalassemia
data$output_cat<-c("0"="low risk","1"="high risk")[data$output]

head(data)



```


# Examine raw distribution of each variable.
```{r}
#age is normally distributed
boxplot(data$age, main = "Distribution of Patients' Ages", ylab = 'Age')
```
```{r}
#the sample has twice as many women as men
plot(data$sex, main = "Distribution of Sex", ylab = 'Frequency', xlab = 'Sex', names = c("Male","Female"))
```

```{r}
#each type of chest pain has a different frequency
plot(data$cp, main = "Distribution of Chest Pain Types", ylab = 'Frequency', xlab = "Chest Pain Type", 
     names = c("Typical Angina", "Atypical Angina", "Non-Anginal Pain", "Asymptomatic"))
```

```{r}
#blood pressure is fairly normally distributed with a few outliers at the top end. Although they are outliers, this data is plausible and can be kept in the dataset as is
boxplot(data$trtbps, main = "Distribution of Resting Blood Pressure", ylab = 'Frequency', xlab = "Resting Blood Pressure (mmHg)")
```
```{r}
#cholesterol is normally distributed with a few outliers at the top end. These are plausible values and can be kept in the dataset
boxplot(data$chol, main = "Distribution of Cholesterol Levels", ylab = 'Cholesterol (mg/dl)')
```

```{r}
#the two groups are disproportionately represented with the majority of the sample being under 120
plot(data$fbs, main = "Fasting Blood Sugar Distribution", ylab = 'Frequency', xlab = 'Blood Sugar', 
     names = c('<= 120 mg/dl','> 120 mg/dl'))  
```     
     
```{r}
#left ventricular hypertrophy is underrepresented 
plot(data$restecg, main = "Distribtion of Resting Electrocardiographic Results", ylab = 'Frequency', xlab = 'ECG Results', 
     names = c("Normal", "ST-T Wave", "L. Vent. Hyper."))
```
```{r}
#normally distributed 
boxplot(data$thalachh, main = "Distribution of Maximum Heart Rate Achieved", ylab = 'Heart Rate (beats per minute)')
```
```{r}
plot(data$exng, main = "Distribution of Exercise-Induced Angina", xlab = 'History of Agina', ylab = 'Frequency', 
     names = c("No", "Yes"))
```

```{r}
#positively skewed
plot(data$caa, main = "Number of Major Vessels Distribution", ylab = 'Frequency', xlab = 'Number of Vessels')
```


```{r}
#the less chance group is slightly underrepresented - balance groups before running classification model
plot(data$output, main = "Distribution of Heart Attack Risk", ylab = 'Frequency', xlab = 'Chance of Heart Attack', 
     names = c("Less Chance", "More Chance"))
```


# Summary of data exploration: there are no true outliers in the data. Can leave the data as is. Some variables were not explained on the Kaggle page, so I left those ones out since I cannot interpret them. Listed below.
## data$thall
## data$oldpeak
## data$slp

# Determine if numerical variables are correlated
```{r}
#run correlation of numerical values
corrs <- cor(data[,c(1,4,5,8)])
corrplot(corrs, method = 'number', tl.cex = .9, number.cex = .9)

#variables are weakly correlated, age and maximum heart rate have strongest correlation -0.4

```

```{r}

head(data, n = 3)

```

# Calculate descriptives
```{r}
#numerical values
mean(data$age)
sd(data$age)
median(data$age)
min(data$age)
max(data$age)

mean(data$trtbps)
sd(data$trtbps)
median(data$trtbps)
min(data$trtbps)
max(data$trtbps)

#median(data$caa)

mean(data$chol)
sd(data$chol)
median(data$chol)
min(data$chol)
max(data$chol)

mean(data$thalachh)
sd(data$thalachh)
median(data$thalachh)
min(data$thalachh)
max(data$thalachh)

#categorical proportions
fx_table2 <- table(data$sex)
prop.table(fx_table2)

fx_table3 <- table(data$exng)
prop.table(fx_table3)

fx_table4 <- table(data$cp)
prop.table(fx_table4)

fx_table5 <- table(data$fbs)
prop.table(fx_table5)

fx_table6 <- table(data$restecg)
prop.table(fx_table6)

```

# Prepping the data for modeling

## Check for Class Imbalance
```{r}
#both classes are well represented, no need to balance 
fx_table <- table(data$output)
prop.table(fx_table)

```

## Splitting the data
```{r}
set.seed(2)
data_split <- createDataPartition(data$output, p = .7, list = FALSE, times = 1)
data_train <- data[data_split,]
data_test <- data[-data_split,]


#verify split proportions
tr_split <- round((dim(data_train)[1]/dim(data)[1])*100,1)
ts_split <- round((dim(data_test)[1]/dim(data)[1])*100,1)
tr_n <- dim(data_train)[1]
ts_n <- dim(data_test)[1]
split_train <- as.data.frame(cbind(tr_split,tr_n))
split_test <- as.data.frame(cbind(ts_split, ts_n))
split_params <- merge(split_train, split_test)
split_params
```

## Baseline Model

```{r}
#Any model that is useful must beat a TPR of 55%
baselinetable<- prop.table(table(data_train$output))
baselinetable

#baseline value assuming all predictictions as positive (1= more chance of heart disease)
accuracy_baseline = round(baselinetable[2]*100,1)


cat("baseline accuracy (all positive/'1' model): ", accuracy_baseline,"\n")
```
```{r}
colnames(data)
```
```{r}
head(data)
```
# Model Required - Binary Classification Model

# Model Testing - Decision Tree CART
```{r}
# Create the formula for the decision tree based on assigned variables
formula <- output ~ age + sex + cp + slp + thall + caa

# Build decision tree model
cart_model <- rpart(formula, data = data_train, method = "class")
summary(cart_model)

# Had the adjust the dimensions of decision tree as it was too crammed
par(mar=c(5,5,2,2))
plot(cart_model, main="Classification Tree", cex=0.7, margin=0.1, branch=0.6) 
text(cart_model, use.n=TRUE, all=TRUE, cex=0.7)

# In these sets of codes, it added number of observations in each leaf
n_leaf <- table(cart_model$where)
n_leaf_pct <- paste0(sprintf("%.2f", prop.table(n_leaf)*100), "%")
n_leaf_text <- paste0(n_leaf, " (", n_leaf_pct, ")")
legend("topright", legend=n_leaf_text, bty="n", cex=0.7)

# Predict on test data
y_pred <- predict(cart_model, data_train, type = "class")

# Applied model on test data
predict_data <- predict(cart_model, data_test[-14], type = 'class')

# Created a table to determine the accuracy of the model
output <- table(data_test$output, predict_data)
output

# Calculate the accuracy of the created model
accuracy_cart <- round((sum(diag(output)) / sum(output)) * 100, 4)
print(paste('The accuracy of the model is:', paste(accuracy_cart), paste('%'), collapse = ' x '))

```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, caa and thall are the greatest predictive attributes
var_importance <- varImp(cart_model)
print(var_importance)
```

```{r}
# Wanted to determine the feature importance using all attributes w/in CART model
# Col names: age, sex, cp, trtbps, chol, fbs, restecg, thalachh, exng, oldpea, slp, caa, thall, output     
# cp_cat, fbs_cat, restecg_cat, exng_cat, slp_cat, thall_cat, output_cat

# Create the formula for the decision tree based on assigned variables
formula <- output ~ age + oldpeak + caa + cp + exng_cat + thall_cat 

# Build decision tree model
cart_model <- rpart(formula, data = data_train, method = "class")
summary(cart_model)

# Had the adjust the dimensions of decision tree as it was too crammed
par(mar=c(5,5,2,2))
plot(cart_model, main="Classification Tree", cex=0.7, margin=0.1, branch=0.6) 
text(cart_model, use.n=TRUE, all=TRUE, cex=0.7)

# In these sets of codes, it added number of observations in each leaf
n_leaf <- table(cart_model$where)
n_leaf_pct <- paste0(sprintf("%.2f", prop.table(n_leaf)*100), "%")
n_leaf_text <- paste0(n_leaf, " (", n_leaf_pct, ")")
legend("topright", legend=n_leaf_text, bty="n", cex=0.7)

# Predict on test data
y_pred <- predict(cart_model, data_train, type = "class")

# Applied model on test data
predict_data <- predict(cart_model, data_test[-14], type = 'class')

# Created a table to determine the accuracy of the model
output <- table(data_test$output, predict_data)
output

# Calculate the accuracy of the created model
accuracy_cart <- round((sum(diag(output)) / sum(output)) * 100, 4)
print(paste('The accuracy of the model is:', paste(accuracy_cart), paste('%'), collapse = ' x '))

```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, caa and thall_cat are the greatest predictive attributes
var_importance <- varImp(cart_model)
print(var_importance)
```

# Model Testing - Decision Tree with C5.0
```{r}
#decision tree
C5_heart <- C5.0(formula = output ~ age + sex + cp + slp + fbs + thall + caa, 
                 data = data_train) 
summary(C5_heart)

#Plot model
plot(C5_heart)

#test set prediction
ypred_c5 <- predict(object = C5_heart, newdata = data_test[,-14])
ypred_c5

# confusion matrix
c5_table <- table(data_test$output, ypred_c5)
rownames(c5_table) <- c('Actual: Less', 'Actual: More')
colnames(c5_table) <- c('Pred: Less', 'Pred: More')
c5_table <- addmargins(A = c5_table, FUN = list(Total = sum), quiet = TRUE)
c5_table

#Evaluation

# Model C5.0: Accuracy
C5_acc <- round(((c5_table[1,1]+c5_table[2,2])/dim(data_test)[1]*100),1)

#Model C5.0: Error rate
C5_err <- round((100 - C5_acc),1)

#Model C5.0: Sensitivity
C5_sens <- round((c5_table[2,2]/(c5_table[2,1]+c5_table[2,2]))*100,1)

#Model C5.0: Specificity
C5_spec <- round((c5_table[1,1]/(c5_table[1,1]+c5_table[1,2]))*100,1)

#Model C5.0: Precision
C5_prec <- round((c5_table[2,2]/(c5_table[2,2]+c5_table[1,2]))*100,1)

#Model C5.0: F(1)
C5_f1 <-  round((2*((C5_prec*C5_spec)/(C5_prec+C5_spec))),1)

#combine evals into a matrix
eval_c5_table <- cbind(C5_acc, C5_err, C5_sens, C5_spec, C5_prec, C5_f1)
eval_c5_table

# Display accuracy
print(paste0("Accuracy: ", C5_acc, "%"))
```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, caa and thall_cat are the greatest predictive attributes
var_importance <- varImp(C5_heart)
print(var_importance)
```

```{r}
# Wanted to see if I could get same or higher accuracy w/ less attributes
#decision tree
#C5_heart <- C5.0(formula = output ~ age + sex + cp + slp + fbs + thall + caa, 
#                 data = data_train) 
C5_heart <- C5.0(formula = output ~ cp + thall_cat + caa, data = data_train) 
summary(C5_heart)

#Plot model
plot(C5_heart)

#test set prediction
ypred_c5 <- predict(object = C5_heart, newdata = data_test[,-14])
ypred_c5

# confusion matrix
c5_table <- table(data_test$output, ypred_c5)
rownames(c5_table) <- c('Actual: Less', 'Actual: More')
colnames(c5_table) <- c('Pred: Less', 'Pred: More')
c5_table <- addmargins(A = c5_table, FUN = list(Total = sum), quiet = TRUE)
c5_table

#Evaluation

# Model C5.0: Accuracy
C5_acc <- round(((c5_table[1,1]+c5_table[2,2])/dim(data_test)[1]*100),1)

#Model C5.0: Error rate
C5_err <- round((100 - C5_acc),1)

#Model C5.0: Sensitivity
C5_sens <- round((c5_table[2,2]/(c5_table[2,1]+c5_table[2,2]))*100,1)

#Model C5.0: Specificity
C5_spec <- round((c5_table[1,1]/(c5_table[1,1]+c5_table[1,2]))*100,1)

#Model C5.0: Precision
C5_prec <- round((c5_table[2,2]/(c5_table[2,2]+c5_table[1,2]))*100,1)

#Model C5.0: F(1)
C5_f1 <-  round((2*((C5_prec*C5_spec)/(C5_prec+C5_spec))),1)

#combine evals into a matrix
eval_c5_table <- cbind(C5_acc, C5_err, C5_sens, C5_spec, C5_prec, C5_f1)
eval_c5_table

# Display accuracy
print(paste0("Accuracy: ", C5_acc, "%"))
```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, cp is the greatest predictive attribute
var_importance <- varImp(C5_heart)
print(var_importance)
```

# Model Testing - Logistic Regression

```{r}
#logistic regression
logreg <- glm(output ~ age + sex + cp + slp + fbs + thall + caa, 
                 data = data_train, family = 'binomial'(link = 'logit'))

summary(logreg)
#plot function to see odds ratio of each predictor
plot_model(logreg, sort.est = TRUE, show.values = TRUE, value.offset = .45, vline.color = 'red',
           axis.lim = c(0.000,35))

#run test data through model
ypred_log <- predict(logreg, newdata = data_test[,-14], type = 'response')
#update output probabilities to binary 0/1
ypred_log <- if_else(ypred_log > 0.5,1,0)

#create contigency
log_table <- table(data_test$output, ypred_log)
rownames(log_table) <- c('Actual: Less', 'Actual: More')
colnames(log_table) <- c('Pred: Less', 'Pred: More')
log_table <- addmargins(A = log_table, FUN = list(Total = sum), quiet = TRUE)
log_table

#evaluation metrics

## Model Log: Accuracy
Log_acc <- round(((log_table[1,1]+log_table[2,2])/dim(data_test)[1]*100),1)

#Model Log: Error rate
Log_err <- round((100 - Log_acc),1)

#Model Log: Sensitivity
Log_sens <- round((log_table[2,2]/(log_table[2,1]+log_table[2,2]))*100,1)

#Model Log: Specificity
Log_spec <- round((log_table[1,1]/(log_table[1,1]+log_table[1,2]))*100,1)

#Model Log: Precision
Log_prec <- round((log_table[2,2]/(log_table[2,2]+log_table[1,2]))*100,1)

#Model Log: F(1)
Log_f1 <-  round((2*((Log_prec*Log_spec)/(Log_prec+Log_spec))),1)

#combine evals into a matrix
eval_log_table <- cbind(Log_acc, Log_err, Log_sens, Log_spec, Log_prec, Log_f1)
eval_log_table

# Display accuracy
print(paste0("Accuracy: ", Log_acc, "%"))
```
```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, cp2 is the greatest predictive attribute
var_importance <- varImp(logreg)
print(var_importance)
```

```{r}
# Testing w/ other attributes
# Original attributes used in the logistic regression
#logreg <- glm(output ~ age + sex + cp + slp + fbs + thall + caa, 
 #                data = data_train, family = 'binomial'(link = 'logit'))
# Updated attributes
logreg <- glm(output ~ sex + cp + chol + exng + slp_cat + caa, 
                 data = data_train, family = 'binomial'(link = 'logit'))

summary(logreg)
#plot function to see odds ratio of each predictor
plot_model(logreg, sort.est = TRUE, show.values = TRUE, value.offset = .45, vline.color = 'red',
           axis.lim = c(0.000,35))

#run test data through model
ypred_log <- predict(logreg, newdata = data_test[,-14], type = 'response')
#update output probabilities to binary 0/1
ypred_log <- if_else(ypred_log > 0.5,1,0)

#create contigency
log_table <- table(data_test$output, ypred_log)
rownames(log_table) <- c('Actual: Less', 'Actual: More')
colnames(log_table) <- c('Pred: Less', 'Pred: More')
log_table <- addmargins(A = log_table, FUN = list(Total = sum), quiet = TRUE)
log_table

#evaluation metrics

## Model Log: Accuracy
Log_acc <- round(((log_table[1,1]+log_table[2,2])/dim(data_test)[1]*100),1)

#Model Log: Error rate
Log_err <- round((100 - Log_acc),1)

#Model Log: Sensitivity
Log_sens <- round((log_table[2,2]/(log_table[2,1]+log_table[2,2]))*100,1)

#Model Log: Specificity
Log_spec <- round((log_table[1,1]/(log_table[1,1]+log_table[1,2]))*100,1)

#Model Log: Precision
Log_prec <- round((log_table[2,2]/(log_table[2,2]+log_table[1,2]))*100,1)

#Model Log: F(1)
Log_f1 <-  round((2*((Log_prec*Log_spec)/(Log_prec+Log_spec))),1)

#combine evals into a matrix
eval_log_table <- cbind(Log_acc, Log_err, Log_sens, Log_spec, Log_prec, Log_f1)
eval_log_table

# Display accuracy
print(paste0("Accuracy: ", Log_acc, "%"))
```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, cp2 is the greatest predictive attribute
var_importance <- varImp(logreg)
print(var_importance)
```

# Model Testing - Random Forest

```{r}

# Build random forest model
rf01 <- randomForest(formula = output ~ age + sex + cp + slp + fbs + thall + caa, data = data_train, ntree = 100, type = 'classification')

summary(rf01)

# Display results
plot(rf01)

# Make predictions on the test data
rf_pred <- predict(rf01, newdata = data_test[,-14])

# Create a confusion matrix
library(caret)
confusionMatrix(rf_pred, data_test$output)

```

```{r}
# The high feature importance score indicates the predictive attributes has on the output
# As evident, cp is the greatest predictive attribute
var_importance <- varImp(rf01)
print(var_importance)
```

# Model Testing - Bayesian Network (Naive Bayes)
```{r}
#bayesian network
library(e1071)
#generate model
nbheart <- naiveBayes(formula = output ~ age + sex + cp + slp + fbs + thall + caa, 
                 data = data_train) 
summary(nbheart)

nb_ypred <- predict(object = nbheart, newdata = data_test[,-14])
#generate confusion matrix
tnb <- table(nb_ypred, data_test$output)

#extract values from confusion matrix to prep for model evaulation
TN_nb <- tnb[1,1]
FN_nb <- tnb[2,1]
FP_nb <- tnb[1,2]
TP_nb <- tnb[2,2] 

#evaluate model
decimals <-1

accuracy_nb <- round((((TP_nb+TN_nb) / (TP_nb + TN_nb + FP_nb + FN_nb)))*100,decimals)
error_rate_nb <- round((100-accuracy_nb),decimals)
sensitivity_nb <- round((TP_nb/(FN_nb+TP_nb))*100,decimals)
specificity_nb <- round((TN_nb/(TN_nb+FP_nb))*100,decimals)
precision_nb <- round((TP_nb/(TP_nb+FP_nb))*100,decimals)
recall_nb <- sensitivity_nb
f1_nb <- round((2*((precision_nb*recall_nb)/(precision_nb + recall_nb))),decimals)
f2_nb <- round((5*((precision_nb*recall_nb)/((4*precision_nb)+recall_nb))),decimals)
f3_nb <- round((1.25*((precision_nb*recall_nb)/((0.25*precision_nb)+ recall_nb))),decimals)

cat("naive bayes accuracy: ", accuracy_nb)
```

# Model Testing - Neural Network

```{r}
nn_heart <-  nnet(output ~ age + sex + cp + slp + fbs + thall + caa, data = data_train, size=20)
yprednn <- predict(nn_heart, data_test[,-14], type="class")

summary(nn_heart)
#visualize neural network
plotnet(nn_heart)

tnn<-table(yprednn,data_test$output)
tnn

TP_nn <- tnn[1,1]
FN_nn <- tnn[2,1]
FP_nn <- tnn[1,2]
TN_nn <- tnn[2,2] 

decimals <-1

accuracy_nn <- round((((TP_nn+TN_nn) / (TP_nn + TN_nn + FP_nn + FN_nn)))*100,decimals)
error_rate_nn <- round((100-accuracy_nn),decimals)
sensitivity_nn <- round((TP_nn/(FN_nn+TP_nn))*100,decimals)
specificity_nn <- round((TN_nn/(TN_nn+FP_nn))*100,decimals)
precision_nn <- round((TP_nn/(TP_nn+FP_nn))*100,decimals)
recall_nn <- sensitivity_nn
f1_nn <- round((2*((precision_nn*recall_nn)/(precision_nn + recall_nn))),decimals)
f2_nn <- round((5*((precision_nn*recall_nn)/((4*precision_nn)+recall_nn))),decimals)
f3_nn <- round((1.25*((precision_nn*recall_nn)/((0.25*precision_nn)+ recall_nn))),decimals)

cat("neural network accuracy: ", accuracy_nn)
```

# Model Evaulation Table

```{r }
DT = data.table(
  Model_Evaluation_Table= c("accuracy", "error rate", "sensitivity", "specificity", "precision", "F1", "F2", "F3"),
  Baseline=c(accuracy_baseline, "","","","","","",""),
  CART= c(accuracy_cart,"error rate", "sensitivity", "specificity", "precision", "F1", "F2", "F3"),
  C5.0=c(C5_acc, C5_err, C5_sens, C5_spec, C5_prec, C5_f1, "F2","F3"),
  LogisticRegression=c("accuracy", "error rate", "sensitivity", "specificity", "precision","F1", "F2", "F3"),
  KNN=c("accuracy", "error rate", "sensitivity", "specificity", "precision", "F1", "F2", "F3"),
  NaiveBayes=c(accuracy_nb, error_rate_nb, sensitivity_nb, specificity_nb, precision_nb,f1_nb, f2_nb, f3_nb),
  NeuralNetwork=c(accuracy_nn, error_rate_nn, sensitivity_nn, specificity_nn, precision_nn, f1_nn, f2_nn, f3_nn))

DT
```
# Comparing ROC curves for all models
```{r}
#data_test$cartpred <- factor(y_pred, ordered = TRUE)
data_test$c50pred <- factor(ypred_c5, ordered = TRUE)
data_test$logpred <- factor(ypred_log, ordered = TRUE)
data_test$rfpred <- factor(rf_pred, ordered = TRUE)
data_test$nbpred <- factor(nb_ypred, ordered = TRUE)
data_test$nnpred <- factor(yprednn, ordered = TRUE)

c50ROC <- roc(data_test$output ~ data_test$c50pred,plot=TRUE,print.auc=TRUE,
              col="green",lwd=4,print.auc.y=0.4,
              main="C5.0 ROC Curve")

logROC <- roc(data_test$output ~ data_test$logpred,plot=TRUE,print.auc=TRUE,
                   col="red",lwd = 4,print.auc.y=0.4,
              main = "Logistic Regression ROC Curve")

rfROC <- roc(data_test$output ~ data_test$rfpred,plot=TRUE,print.auc=TRUE,
                   col="orange",lwd = 4,print.auc.y=0.4,
             main = "Random Forest ROC Curve")

nbROC <- roc(data_test$output ~ data_test$nbpred,plot=TRUE,print.auc=TRUE,
                   col="purple",lwd = 4,print.auc.y=0.4,
             main = "Naive Bayes ROC Curve")

nnROC <- roc(data_test$output ~ data_test$nnpred,plot=TRUE,print.auc=TRUE,
                   col="pink",lwd = 4,print.auc.y=0.4,
             main = "Neural Network ROC Curve")
```


# References

## Joachim Schork. (2022, May 19). Create data.table in R (3 examples): How to initialize, Construct &amp; Make. Statistics Globe. Retrieved March 26, 2023, from https://statisticsglobe.com/create-data-table-r 
## Kuhn, M. (2019). The caret package. https://topepo.github.io/caret/index.html
## Larose, C., & Larose, D. (2019). Data Science Using Python and R. John Wiley & Sons, Inc. 
## Daniel Lüdecke. (2023). Plotting Estimates (Fixed Effects) of Regression Models. https://strengejacke.github.io/sjPlot/articles/plot_model_estimates.html
## Ruchi Deshpande. (2020). ROC Curve and AUC in Machine learning and R pROC Package. https://medium.com/swlh/roc-curve-and-auc-detailed-understanding-and-r-proc-package-86d1430a3191

